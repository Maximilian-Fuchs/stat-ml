{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book: Introduction to Statistical Learning\n",
    "[amazon link](https://www.amazon.de/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370)\n",
    "\n",
    "---\n",
    "### Notation\n",
    "Datasets are denoted as $\\textbf{X} = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{pmatrix}\n",
    "$ where $n$ is the number of *examples* and $p$ is the number of *features*.\n",
    "\n",
    "*Features* are called *variables* in the book.\n",
    "\n",
    "missing part... because my laptops battery died and I didn't save :(\n",
    "\n",
    "---\n",
    "\n",
    "### Inference\n",
    "\n",
    "*Inference* is the statistical term for *Data Mining*.\n",
    "Instead of building a *model* with the goal of *prediction* the model is built with the goal to understand how $X_1, ..., X_p$ affect $Y$.\n",
    "*f* can not be treated as a black box because we need to know its exact form.\n",
    "Following question may be answered:\n",
    "- Which are the most relevant features for a decision?\n",
    "- What is the relationship (e.g. positive/negative) between *response* ($Y$) and *predictor* ($X_1, ..., X_p$)\n",
    "\n",
    "##### Example\n",
    "In Advertising: Where the *dataset* consists of *features* in form of spendings on advertisement per media and *labels* in form of the amount of sales.\n",
    "-  Which media contributes most to sales.\n",
    "- How much increase in sales is associated with a given increase in TV advertisement?\n",
    "---\n",
    "## How do we estimate $f$?\n",
    "\n",
    "### Parametric Methods\n",
    "2 Steps:\n",
    "  1. We make an assumption about the form or shape of $f$ (the *model*). FOr example that $f$ is linear in $X$: $$f(X)=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\ldots+\\beta_pX_p$$. Once we assumed that $f$ is linear, all we have to do is to estimate the coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_p$\n",
    "  2. After a *model* has been selected we nee a procedure or *learning algorithm* that uses the *training data* to *fit* or *train* the *model*. That is, we want to estimate the parameters $\\beta_0, \\beta_1, \\ldots, \\beta_p$. That is, we want to find values of these parameters such that $$Y\\approx \\beta_0+\\beta_1X_1+\\beta_2X_2+\\ldots+\\beta_pX_p$$. The most common approach referred to as *(ordinary) least squares*\n",
    "\n",
    "The *parametric* approach to estimating the *model* $f$ reduces the problem down to estimating a set of parameters, which is much easyer than the problem of estimating an entirely arbitrary function $f$.\n",
    "\n",
    "### Non-parametric methods\n",
    "\n",
    "- Non-parametric methods do not make explicit asumptions about the functional form or shape of $f$.\n",
    "- They serve as a universal function approximator, therefore they can potentially describe data way better than models that are constrained by an assumption.\n",
    "- **But**\n",
    "\t- They need more data to be trained. I.e. Far more observations are needed in order to obtain an accurate estimate for $f$.\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
